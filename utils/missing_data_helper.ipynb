{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 50\n",
    "pd.set_option(\"expand_frame_repr\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "class data_preprocesser:\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        # Preprocessing for numerical data\n",
    "        numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "        # Preprocessing for categorical data\n",
    "        categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))]) \n",
    "\n",
    "        # Bundle preprocessing for numerical and categorical data\n",
    "        preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_missing_pct(train, test, features, show_all=False):\n",
    "    '''\n",
    "    Method for calculating the missing percentages from both test and train datasets\n",
    "    given a list of features to include\n",
    "    Option for showing all features or only those with missing values (Default)\n",
    "    Params:\n",
    "        ► train (Pandas DataFrame) | training data set\n",
    "        ► test (Pandas DataFrame) | testing dataset\n",
    "        ► features (list) | List of features (Column names (str))\n",
    "        ► show_al (bool) | Optional True/False for showing all features or only those with missing values (Default hide)\n",
    "    Return:\n",
    "        ► Pandas DataFrame with column names as the index and 2 columns representing train and test dataset missing percentages\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Calculate the missing percentages for both train and test data\n",
    "    train_missing_pct = train[features].isnull().mean() * 100\n",
    "    test_missing_pct = test[features].isnull().mean() * 100\n",
    "\n",
    "    # Combine the missing percentages for train and test data into a single dataframe\n",
    "    missing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train%', 'Test%'])\n",
    "\n",
    "    # If show_all is True, then print full dataframe, else print a filtered one showing only non-zero percentages\n",
    "    if not show_all:\n",
    "\n",
    "        # Print the missing percentage dataframe (Excl. features without missing values)\n",
    "        missing_features = missing_pct_df[(missing_pct_df['Train%'] > 0) | (missing_pct_df['Test%'] > 0)]\n",
    "    else:\n",
    "        missing_features = missing_pct_df\n",
    "    \n",
    "    print(missing_features)\n",
    "    \n",
    "    return missing_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(model, X_train, X_valid, y_train, y_valid):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def drop_cols(df):\n",
    "\n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
    "\n",
    "    # Drop columns in training and validation data\n",
    "    return df.drop(cols_with_missing, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_cols(df):\n",
    "    \n",
    "    # Imputation\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_df = pd.DataFrame(my_imputer.fit_transform(df))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_df.columns = df.columns  \n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_extend(df):\n",
    "    # Make copy to avoid changing original data (when imputing)\n",
    "    df_plus = df.copy()\n",
    "    \n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
    "\n",
    "    # Make new columns indicating what will be imputed\n",
    "    for col in cols_with_missing:\n",
    "        df_plus[col + '_was_missing'] = df_plus[col].isnull()\n",
    "    \n",
    "    # Imputation\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_df_plus = pd.DataFrame(my_imputer.fit_transform(df_plus))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_df_plus.columns = df_plus.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_cat_columns_short:\n",
    "    \n",
    "    cardinality_limit = 10\n",
    "\n",
    "    def __init__(self, train, test, cardinality_limit=10):\n",
    "\n",
    "        # Categorical columns in the training data\n",
    "        self.object_cols = [col for col in train.columns if train[col].dtype == \"object\"]\n",
    "\n",
    "        # Columns that can be safely ordinal encoded (If columns exist within both datasets)\n",
    "        self.good_label_cols = [col for col in self.object_cols if \n",
    "                        set(test[col]).issubset(set(train[col]))]\n",
    "                \n",
    "        # Problematic columns that will be dropped from the dataset\n",
    "        self.bad_label_cols = list(set(self.object_cols)-set(self.good_label_cols))\n",
    "\n",
    "        # Columns that will be one-hot encoded\n",
    "        self.low_cardinality_cols = [col for col in self.object_cols if train[col].nunique() < cardinality_limit]\n",
    "\n",
    "        # Columns that will be dropped from the dataset\n",
    "        self.high_cardinality_cols = list(set(self.object_cols)-set(self.low_cardinality_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def apply_one_hot_encoder(train, test):\n",
    "\n",
    "    cl = get_cat_columns_short(train, test)\n",
    "\n",
    "    # Categorical columns in the training data\n",
    "    object_cols = cl.object_cols\n",
    "\n",
    "    # Columns that can be safely ordinal encoded (If columns exist within both datasets)\n",
    "    good_label_cols = cl.good_label_cols\n",
    "            \n",
    "    # Problematic columns that will be dropped from the dataset\n",
    "    bad_label_cols = cl.bad_label_cols\n",
    "\n",
    "    # Columns that will be one-hot encoded\n",
    "    low_cardinality_cols = cl.low_cardinality_cols\n",
    "\n",
    "    # Columns that will be dropped from the dataset\n",
    "    high_cardinality_cols = cl.high_cardinality_cols\n",
    "\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[low_cardinality_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(test[low_cardinality_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = train.index\n",
    "    OH_cols_valid.index = test.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = train.drop(object_cols, axis=1)\n",
    "    num_X_valid = test.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "    # Ensure all columns have string type\n",
    "    OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "    OH_X_valid.columns = OH_X_valid.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
